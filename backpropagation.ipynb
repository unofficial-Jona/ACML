{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from typing import List, Union, Callable\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.identity(8)\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def quadratic_loss(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    norms = np.apply_along_axis(np.linalg.norm, 0, predictions-actuals)\n",
    "    return 0.5*np.apply_along_axis(np.power, 0, norms, 2)\n",
    "\n",
    "def quadratic_loss_derivative(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    return -(actuals-predictions)\n",
    "\n",
    "def binary_crossentropy(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    return actuals*np.log(predictions)+(np.ones(actuals.shape)-actuals)*np.log(np.ones(predictions.shape)-predictions)\n",
    "\n",
    "def cost_function(loss_function: Callable, predictions: np.ndarray, actuals: np.ndarray, weights: List[np.ndarray], decay_parameter: float) -> float:\n",
    "    avg_loss: float = np.mean(loss_function(predictions, actuals))\n",
    "    sum_layers = [np.power(layer_weight, 2).sum() for layer_weight in weights]\n",
    "    sum_all_weights = sum(sum_layers)\n",
    "    regularization: float = 0.5*decay_parameter*sum_all_weights\n",
    "    return avg_loss+regularization\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    weigths: np.ndarray\n",
    "    has_bias: bool\n",
    "\n",
    "    def __init__(self, num_nodes: int, num_nodes_n1: int, include_bias: bool = True, epsilon: float = 0.01) -> None:\n",
    "        self.weights = np.random.normal(loc=0, scale=np.power(epsilon,2), size=(num_nodes_n1, np.add(num_nodes, include_bias)))\n",
    "        self.has_bias = include_bias\n",
    "    \n",
    "    def print_weights(self) -> None:\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    layers: List[Layer]\n",
    "    weights: List[np.ndarray]\n",
    "\n",
    "    def __init__(self, num_nodes: List[int], include_biases: List[bool]) -> None:\n",
    "        # num_nodes is a list of number of nodes for all layers not counting the bias node\n",
    "        # TO-DO: code won't work if include_biases != [True, True, False], see prop_forward\n",
    "        assert (include_biases == [True, True, False]), 'error when initializing Network class: include_bias parameter not available'\n",
    "\n",
    "        self.layers = [Layer(num_nodes[i], num_nodes[i+1], include_biases[i]) for i in range(len(num_nodes)-1)]\n",
    "        self.layers.append(Layer(num_nodes[-1], 0, include_biases[-1]))\n",
    "        self.weights = self.get_weights(form='list')\n",
    "\n",
    "    def get_weights(self, form: str = 'vector') -> Union[List[np.ndarray], np.ndarray]:\n",
    "        assert (form in ['vector', 'list']), 'Error in get_weights function: form parameter ill-defined'\n",
    "\n",
    "        if form == 'vector':\n",
    "            # returns one np.ndarray with all weights of all layers\n",
    "            weigth_vector = []\n",
    "            for layer in self.layers:\n",
    "                weigth_vector.append(layer.weights)\n",
    "            return np.asarray(weigth_vector)\n",
    "        elif form == 'list':\n",
    "            # returns a list, where list[i] stores the weights between layer i-1 and layer i\n",
    "            list_of_weights: List[np.ndarray] = []\n",
    "            for layer in self.layers:\n",
    "                list_of_weights.append(layer.weights)\n",
    "            return list_of_weights\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        print('Printing weights of network:')\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f'Layer {index+1}')\n",
    "            layer.print_weights()\n",
    "\n",
    "    def prop_forward(self, features: np.ndarray) -> List[np.ndarray]:\n",
    "        # returns a list, where list[i] stores the activations for neurons in layer i+1\n",
    "        # the activation of a bias node (should the layer have one) is given by the first value in the array and is always =1\n",
    "        # TO-DO: right now it is hard coded that input layer & hidden layer have a bias node, but output layer has not\n",
    "        z_2 = np.matmul(self.weights[0], np.append(1, features))\n",
    "        a_2 = np.apply_along_axis(sigmoid, 0, z_2)\n",
    "        z_3 = np.matmul(self.weights[1], np.append(1, a_2))\n",
    "        a_3 = np.apply_along_axis(sigmoid, 0, z_3)\n",
    "        return [np.append(1, features), np.append(1, a_2), a_3]\n",
    "\n",
    "    def print_activations(self, features: np.ndarray) -> None:\n",
    "        print(f'Printing activations for input: {features}')\n",
    "        for index, array in enumerate(self.prop_forward(features)):\n",
    "            print(f'Layer {index+1}: {array}')\n",
    "\n",
    "    def calculate_z(self, num_layer: int, activations: List[np.ndarray]) -> float:\n",
    "        z = np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "        return np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "\n",
    "    def get_deltas(self, X: np.ndarray, y: np.ndarray, loss_function_derivative: Callable, activations: List[np.ndarray]=None) -> List[np.ndarray]:\n",
    "        # TO-DO: adapt code to accept different cost functions\n",
    "        # Right now: hard coded to use quadratic loss\n",
    "        if activations == None:\n",
    "            activations = self.prop_forward(X)\n",
    "        deltas = []\n",
    "        # Fehler: activations != z!!!!\n",
    "        # deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, activations[-1]))\n",
    "        deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z(3, activations)))\n",
    "        deltas.insert(0, deltas_output)\n",
    "        for i in range(len(activations)-2):\n",
    "            # delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, activations[-2]))\n",
    "            delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)][:,1:]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z((len(activations)-1-i), activations)))\n",
    "            # remove 'bias delta', as activation of bias cannot be changed\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "\n",
    "    def partial_derivatives(self, X: np.ndarray, y: np.ndarray, verbose: bool =False) -> List[np.ndarray]:\n",
    "        activations = self.prop_forward(X)\n",
    "        deltas = self.get_deltas(X, y, activations)\n",
    "        partial_derivatives = []\n",
    "        for index in range(len(deltas)):\n",
    "            if verbose:\n",
    "                # for testing/debugging purposes\n",
    "                print(f'Layer {index+1}: dimension deltas {index+2} {deltas[index].shape}, dimension activations {index+1} {activations[index].shape}')\n",
    "                print(f'activations: {activations[index]}')\n",
    "                print(f'deltas: {deltas[index]}')\n",
    "            partial = np.outer(deltas[index], np.transpose(activations[index]))\n",
    "            if verbose:\n",
    "                # deltas should be equal to partial derivatives of the bias node\n",
    "                print(f'Partial of bias: {partial[:,0]}')\n",
    "            partial_derivatives.append(partial)\n",
    "        return partial_derivatives\n",
    "\n",
    "    def update_weights(self, big_delta: List[np.ndarray], regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        for num_layer in range(len(self.weights)):\n",
    "            self.weights[num_layer] = self.weights[num_layer]-learning_rate*(big_delta[num_layer] + regularization_parameter*self.weights[num_layer])\n",
    "\n",
    "    def gradient_descent(self, X_train: np.ndarray, y_train: np.ndarray, regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        big_delta: List[np.ndarray] = [] \n",
    "        for index in range(len(self.weights)):\n",
    "            big_delta.append(np.zeros(self.weights[index].shape))\n",
    "        for num_instance in range(X_train.shape[0]):\n",
    "            partials = self.partial_derivatives(X[num_instance], y[num_instance])\n",
    "            for num_layer in range(len(self.layers)-1):\n",
    "                big_delta[num_layer] = big_delta[num_layer]+partials[num_layer]\n",
    "        for num_layer in range(len(self.layers)):\n",
    "            big_delta[num_layer] = (1/X_train.shape[0])*big_delta[num_layer]\n",
    "        self.update_weights(big_delta, regularization_parameter, learning_rate)\n",
    "\n",
    "    def train_network(self, n_iter: int, X_train: np.ndarray, y_train: np.ndarray, regul_param: float, learning_rate: float=0.01) -> None:\n",
    "        # TO-DO: print/plot loss after each iteration \n",
    "        \n",
    "        # calculate activation\n",
    "        # loss based on activation\n",
    "        costs = [[], []]\n",
    "        for iter in range(n_iter):\n",
    "            \n",
    "            temp_costs = []            \n",
    "            \n",
    "            for instance in X_train:\n",
    "                act_3 = self.prop_forward(instance)[2].reshape((8,1))\n",
    "                # print('shape act_3: ', act_3.shape, ' shape weights_3: ', weights_3.shape)\n",
    "                cos = cost_function(quadratic_loss, act_3, instance, self.weights, regul_param)\n",
    "                temp_costs.append(cos)\n",
    "                # print('predictions: ', act_3)\n",
    "            costs[0].append(iter)\n",
    "            costs[1].append(np.mean(temp_costs))\n",
    "            self.gradient_descent(X_train, y_train, regularization_parameter=regul_param, learning_rate=learning_rate)\n",
    "            \n",
    "        \n",
    "        plt.plot(costs[0], costs[1])\n",
    "        plt.show()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_network.print_weights()\n",
    "#test_network.print_activations(X[0])\n",
    "#print(test_network.get_deltas(X[0], X[0]))\n",
    "#print(test_network.partial_derivatives(X[0], X[0], verbose=False)[0])\n",
    "#test_network.gradient_descent(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVXklEQVR4nO3dfYxc13nf8e8zs7sUJVKiZG5kmaREqqWSMEklK2vVQetUgB2HMuKwTluDagEbbgJCrVXEDVpEqQE3QIG2qZsAdaWEUFtBcZtaQRC7Zgo6UpGkdlvYiVYKJYuSaK8ky1qTlihR75S1XPLpH3OXnLflzi5nZ3iG3w8wmJlz78x9eGf42zPnvkVmIkkqX23YBUiS+sNAl6QRYaBL0ogw0CVpRBjokjQixoa14I0bN+bWrVuHtXhJKtJDDz30YmZOdps2tEDfunUr09PTw1q8JBUpIp5dbJpDLpI0Igx0SRoRBrokjQgDXZJGhIEuSSNiyUCPiHsi4oWIeGyR6RERn4uImYh4NCJu7H+ZkqSl9NJDvxfYeZbptwDbq9se4HfOvSxJ0nItGeiZ+TXg2Flm2QV8Phu+AWyIiKv6VWC7Q99/nd964BAvvvH2ai1CkorUjzH0TcBzTc9nq7YOEbEnIqYjYvro0aMrWtjMC2/wuT+d4dibcyt6vSSNqn4EenRp63rVjMy8OzOnMnNqcrLrkatLqlVLO+WFOSSpRT8CfRbY0vR8M3C4D+/bVUQj0U+dWq0lSFKZ+hHo+4CPVXu7vBd4NTOP9OF9u7KHLkndLXlyroj4AnAzsDEiZoF/CYwDZOZeYD/wIWAGOA58YrWKBagt9NANdElqsWSgZ+atS0xP4JN9q2gJteo3xSnzXJJaFHekqD10Sequ2EBPA12SWhQb6A65SFKrAgO9cX/KRJekFsUF+sJ+6CcdcpGkFsUFer22MIY+5EIk6TxTXKB7YJEkdVdcoIcbRSWpq+IC3R66JHVXYKC7H7okdVNsoJ/0bIuS1KK8QD99Lhd76JLUrLxAd8hFkroqNtDdy0WSWhUY6I17h1wkqVVxgX760H+76JLUorhAX+ih20GXpFbFBfrCuVwccpGkVsUFuhtFJam74gI93CgqSV0VF+juhy5J3RUb6B76L0mtygt0D/2XpK7KC3SHXCSpq2ID3b1cJKlVgYHeuHfIRZJaFRfoXoJOkrrrKdAjYmdEHIqImYi4o8v0yyPiSxHxaET8RUT8eP9LbTjdQzfRJanFkoEeEXXgLuAWYAdwa0TsaJvtXwAHMvOvAR8D/kO/C13gof+S1F0vPfSbgJnMfDoz54D7gF1t8+wA/gQgM58EtkbElX2ttOJGUUnqrpdA3wQ81/R8tmpr9gjwCwARcRNwDbC5/Y0iYk9ETEfE9NGjR1dUsIf+S1J3vQR6dGlrT9N/C1weEQeAfwL8JTDf8aLMuzNzKjOnJicnl1sr4H7okrSYsR7mmQW2ND3fDBxuniEzXwM+ARCN3VCeqW5956H/ktRdLz30B4HtEbEtIiaA3cC+5hkiYkM1DeCXgK9VId937ocuSd0t2UPPzPmIuB24H6gD92TmwYi4rZq+F/hR4PMRcRJ4HPjF1So4IohwyEWS2vUy5EJm7gf2t7XtbXr8dWB7f0tbXC3CvVwkqU1xR4pCY9jFIRdJalVkoIc9dEnqUGSg20OXpE5FBno9wnO5SFKbIgPdjaKS1KnIQA+HXCSpQ5GBXquF+6FLUpsyAz2Ckwa6JLUoNNA9fa4ktSs00B1ykaR2xQb6Kc+2KEktCg1093KRpHZFBrqH/ktSpyIDvVazhy5J7YoM9HqEgS5JbYoMdA/9l6RORQa6h/5LUqciA9390CWpU7GB7n7oktSqyECPwHO5SFKbIgO97tkWJalDkYHuXi6S1KnQQHcvF0lqV2Sge+i/JHUqMtBrgReJlqQ2hQa6h/5LUrueAj0idkbEoYiYiYg7uky/LCL+KCIeiYiDEfGJ/pd6Rq1moEtSuyUDPSLqwF3ALcAO4NaI2NE22yeBxzPzeuBm4DcjYqLPtZ7mJegkqVMvPfSbgJnMfDoz54D7gF1t8ySwPiICWAccA+b7WmkTD/2XpE69BPom4Lmm57NVW7M7gR8FDgPfBH45MzsOzo+IPRExHRHTR48eXWHJ7ocuSd30EujRpa09Tn8WOAC8C7gBuDMiLu14UebdmTmVmVOTk5PLLPWMWi2YN9ElqUUvgT4LbGl6vplGT7zZJ4AvZsMM8AzwI/0psVPd3RYlqUMvgf4gsD0itlUbOncD+9rm+S7wfoCIuBL4YeDpfhbarF6rcdJAl6QWY0vNkJnzEXE7cD9QB+7JzIMRcVs1fS/wr4B7I+KbNIZofjUzX1y1omthoEtSmyUDHSAz9wP729r2Nj0+DHywv6Utrl4L5j0huiS1KPJI0XrNvVwkqV2xgW4PXZJaFRvo5rkktSoz0MMeuiS1KzPQ68FJ81ySWpQZ6BGctIcuSS3KDHT3Q5ekDga6JI2IIgN9zJNzSVKHIgO97hWLJKlDsYFuD12SWhUb6JmeQleSmpUZ6NG45sZJh10k6bQyA71eBbo9dEk6rcxADwNdktqVGeg1h1wkqV3ZgX7SQJekBUUG+lgV6O66KElnFBno9VqjbA8ukqQzCg30xr09dEk6o9BAr3roBroknVZooDfu7aFL0hmFBnqjbPdDl6Qzygx0DyySpA5lBnrNQJekdga6JI2IIgP9zIFFXihakhb0FOgRsTMiDkXETETc0WX6P4+IA9XtsYg4GRFX9L/choUeugcWSdIZSwZ6RNSBu4BbgB3ArRGxo3mezPxsZt6QmTcAvwZ8NTOPrUK9wJlAn/dcLpJ0Wi899JuAmcx8OjPngPuAXWeZ/1bgC/0objGebVGSOvUS6JuA55qez1ZtHSLiYmAn8IeLTN8TEdMRMX306NHl1nqaG0UlqVMvgR5d2hZL0g8D/2+x4ZbMvDszpzJzanJystcaOxjoktSpl0CfBbY0Pd8MHF5k3t2s8nALeGCRJHXTS6A/CGyPiG0RMUEjtPe1zxQRlwF/C/hyf0vsZA9dkjqNLTVDZs5HxO3A/UAduCczD0bEbdX0vdWsHwEeyMw3V63aioEuSZ2WDHSAzNwP7G9r29v2/F7g3n4VdjZj7uUiSR2KPFK0Zg9dkjoUGehjHlgkSR2KDHTH0CWpU5GBPlZd4MIrFknSGUUG+ni90UM/cdKzLUrSgjIDfaxRtoEuSWcUGegT9YVAd8hFkhYUGejjdXvoktSuyECv14IIA12SmhUZ6NDopc8Z6JJ0WrGBPlGveWCRJDUpNtDH6+GQiyQ1KTjQawa6JDUpOtDn5h1ykaQFBQe6Qy6S1KzgQK8xf8pAl6QFRQe6Qy6SdEa5gT7mRlFJalZuoNccQ5ekZuUGurstSlKLcgN9rObZFiWpSbGBPuFui5LUothAd8hFkloVG+hjdYdcJKlZsYE+Xg/m5u2hS9KCYgN9wiEXSWrRU6BHxM6IOBQRMxFxxyLz3BwRByLiYER8tb9ldmoc+u+QiyQtGFtqhoioA3cBPwPMAg9GxL7MfLxpng3AbwM7M/O7EfFDq1TvaeP1GicccpGk03rpod8EzGTm05k5B9wH7Gqb5+8DX8zM7wJk5gv9LbPTeD28BJ0kNekl0DcBzzU9n63aml0HXB4R/zsiHoqIj/WrwMV4TVFJarXkkAsQXdraB6/HgJ8E3g+sBb4eEd/IzG+1vFHEHmAPwNVXX738apusGauRCSdOnmK8Xuy2XUnqm16ScBbY0vR8M3C4yzx/nJlvZuaLwNeA69vfKDPvzsypzJyanJxcac0AXDReB+AHJ06e0/tI0qjoJdAfBLZHxLaImAB2A/va5vky8L6IGIuIi4G/DjzR31JbXTTeKP0tA12SgB6GXDJzPiJuB+4H6sA9mXkwIm6rpu/NzCci4o+BR4FTwH/OzMdWs/CFHvrbJxxHlyTobQydzNwP7G9r29v2/LPAZ/tX2tk55CJJrYrdmngm0O2hSxIUHeiOoUtSs2IDfa1DLpLUothAdwxdkloVHOiN0n/g+VwkCSg40NeMVT30OXvokgQFB/raiSrQ5w10SYKCA90xdElqVW6gj1Vj6O6HLklAwYE+Vq8xVgv3Q5ekSrGBDo190R1ykaSGogN9jYEuSacVHejr1tR5820DXZKg9EC/aIzXf3Bi2GVI0nmh6EBfv2acN96eH3YZknReKDrQGz10A12SoPBAX79mzB66JFWKDvR1FxnokrSg6EBfXw25ZOawS5GkoSs60NetGefkqfTwf0mi9EC/qHGN69ffdtdFSSo60NevaQT6G+7pIkmFB3rVQ3/NQJeksgN9w8UTALx8fG7IlUjS8BUd6BvXNQL9pTcMdEkqOtCvuKQR6MfefHvIlUjS8BUd6OvWjDFRr9lDlyR6DPSI2BkRhyJiJiLu6DL95oh4NSIOVLfP9L/UrnXxjnUTvPSmgS5JY0vNEBF14C7gZ4BZ4MGI2JeZj7fN+n8y8+dWocazuuKSCY4Z6JLUUw/9JmAmM5/OzDngPmDX6pbVuysumeDFNxxDl6ReAn0T8FzT89mqrd1PRcQjEfGViPixbm8UEXsiYjoipo8ePbqCcjtdeelFPP/aD/ryXpJUsl4CPbq0tZ8N62Hgmsy8HviPwP/o9kaZeXdmTmXm1OTk5LIKXcymDWt54fW3mZv3fC6SLmy9BPossKXp+WbgcPMMmflaZr5RPd4PjEfExr5VeRabLl9LJhx59a1BLE6Szlu9BPqDwPaI2BYRE8BuYF/zDBHxzoiI6vFN1fu+1O9iu9m8YS0A33vZQJd0YVtyL5fMnI+I24H7gTpwT2YejIjbqul7gb8L/KOImAfeAnbngE5SvunyRqDPvmKgS7qwLRnocHoYZX9b296mx3cCd/a3tN68a8NaxuvBMy++OYzFS9J5o+gjRQHG6zWu3biOb33/9WGXIklDVXygA1z3zvUcet5Al3RhG4lA/5F3rmf25bd49S2vXCTpwjUSgX7j1ZcDMP2dY0OuRJKGZyQC/d1Xb2CiXuPPnzHQJV24RiLQLxqvc8OWDfzfb7847FIkaWhGItABPvhjV/L4kdd46ugbwy5FkoZiZAL9w9e/iwj4g+nZYZciSUMxMoF+5aUXccuPv5P/9o1nednzo0u6AI1MoAN86gPX8daJk3xm30EGdOYBSTpvjFSgX3flev7pB7bzR48c5t8/cMhQl3RB6elcLiX5xzf/Vb73ylvc9WdP8fCzr/ArH7yOqWsupzoZpCSNrJEL9Fot+Ncf+Qmu37yBf/OVJ/l7e7/Opg1ree+17+C6K9exdeMlbFw3wYaLJ7hs7ThrxmpMjNWYqNcMfUlFi2ENS0xNTeX09PSqLuP43Dz/85Ej/OmTLzD97MtLXnt0vB6M12vUIhqXaYrG5Zoigmh+3NEG0fXCTr1Z6d+Rc/nzM4w/Xiv+d55DqSv9XEr6TM7pkxxyH2aYix9mB273e7bwS++7dkWvjYiHMnOq27SR66E3u3hijI++ZwsffU/jgkuvHj/Bd48d59jxOV45Pscrx08wN3+KuZOnTt+fmD/FqYQkWfhbl5kkkE3tudCecC5/E7Pjan49vu6clrnC1w3h37niYs/hpSvt5JxL12il6/bcljncbUxDXfqQN69tXLdmVd53pAO93WUXj/MTF1827DIkaVWM1F4uknQhM9AlaUQY6JI0Igx0SRoRBrokjQgDXZJGhIEuSSPCQJekETG0Q/8j4ijw7ApfvhE4H683d77WBedvbda1PNa1PKNY1zWZOdltwtAC/VxExPRi5zIYpvO1Ljh/a7Ou5bGu5bnQ6nLIRZJGhIEuSSOi1EC/e9gFLOJ8rQvO39qsa3msa3kuqLqKHEOXJHUqtYcuSWpjoEvSiCgu0CNiZ0QcioiZiLhjwMveEhF/FhFPRMTBiPjlqv3XI+J7EXGgun2o6TW/VtV6KCJ+dhVr+05EfLNa/nTVdkVE/K+I+HZ1f/kg64qIH25aJwci4rWI+NQw1ldE3BMRL0TEY01ty14/EfGT1XqeiYjPxTlex2yRuj4bEU9GxKMR8aWI2FC1b42It5rW294B17Xsz21Adf1+U03fiYgDVfsg19di2TDY71jjMmpl3IA68BRwLTABPALsGODyrwJurB6vB74F7AB+HfhnXebfUdW4BthW1V5fpdq+A2xsa/t3wB3V4zuA3xh0XW2f3feBa4axvoCfBm4EHjuX9QP8BfBTNC6H+RXgllWo64PAWPX4N5rq2to8X9v7DKKuZX9ug6irbfpvAp8ZwvpaLBsG+h0rrYd+EzCTmU9n5hxwH7BrUAvPzCOZ+XD1+HXgCWDTWV6yC7gvM9/OzGeAGRr/hkHZBfxu9fh3gb89xLreDzyVmWc7OnjV6srMrwHHuiyv5/UTEVcBl2bm17PxP+/zTa/pW12Z+UBmzldPvwFsPtt7DKqusxjq+lpQ9WQ/CnzhbO+xSnUtlg0D/Y6VFuibgOeans9y9kBdNRGxFXg38OdV0+3VT+R7mn5WDbLeBB6IiIciYk/VdmVmHoHGFw74oSHUtWA3rf/Rhr2+YPnrZ1P1eFD1AfxDGr20Bdsi4i8j4qsR8b6qbZB1LedzG/T6eh/wfGZ+u6lt4OurLRsG+h0rLdC7jSUNfL/LiFgH/CHwqcx8Dfgd4K8ANwBHaPzsg8HW+zcy80bgFuCTEfHTZ5l3oOsxIiaAnwf+oGo6H9bX2SxWx6DX26eBeeD3qqYjwNWZ+W7gV4D/HhGXDrCu5X5ug/48b6W10zDw9dUlGxaddZEazqm20gJ9FtjS9HwzcHiQBUTEOI0P7Pcy84sAmfl8Zp7MzFPAf+LMMMHA6s3Mw9X9C8CXqhqer37CLfzMfGHQdVVuAR7OzOerGoe+virLXT+ztA5/rFp9EfFx4OeAf1D99Kb6ef5S9fghGuOu1w2qrhV8boNcX2PALwC/31TvQNdXt2xgwN+x0gL9QWB7RGyren27gX2DWng1RvdfgCcy87ea2q9qmu0jwMIW+H3A7ohYExHbgO00Nnj0u65LImL9wmMaG9Ueq5b/8Wq2jwNfHmRdTVp6TsNeX02WtX6qn8yvR8R7q+/Cx5pe0zcRsRP4VeDnM/N4U/tkRNSrx9dWdT09wLqW9bkNqq7KB4AnM/P0cMUg19di2cCgv2PnsmV3GDfgQzS2ID8FfHrAy/6bNH7+PAocqG4fAv4r8M2qfR9wVdNrPl3Veohz3JJ+lrqupbHF/BHg4MJ6Ad4B/Anw7er+ikHWVS3nYuAl4LKmtoGvLxp/UI4AJ2j0gn5xJesHmKIRZE8Bd1Idbd3numZojK8ufMf2VvP+nerzfQR4GPjwgOta9uc2iLqq9nuB29rmHeT6WiwbBvod89B/SRoRpQ25SJIWYaBL0ogw0CVpRBjokjQiDHRJGhEGuiSNCANdkkbE/we3Ecw+8CHIOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network = Network([8,3,8], [True, True, False])\n",
    "test_network.train_network(2000, X, y, regul_param=0.001, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 0., 0., 0., 0., 0., 0., 0.]), array([1.        , 0.66868968, 0.6687367 , 0.668899  ]), array([0.13188984, 0.13189048, 0.13189055, 0.13189194, 0.13188964,\n",
      "       0.13188975, 0.13189099, 0.13188936])]\n"
     ]
    }
   ],
   "source": [
    "print(test_network.prop_forward(X[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06a35f6432bcea124c520d36814be75a6dd5ed4335e0c829924d510b7f0b7dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
