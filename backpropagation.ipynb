{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from typing import List, Union, Callable\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.identity(8)\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def quadratic_loss(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    norms = np.apply_along_axis(np.linalg.norm, 0, predictions-actuals)\n",
    "    return 0.5*np.apply_along_axis(np.power, 0, norms, 2)\n",
    "\n",
    "def quadratic_loss_derivative(predictions: np.ndarray, actuals: np.ndarray) -> np.ndaraay:\n",
    "    return -(actuals-predictions)\n",
    "\n",
    "def binary_crossentropy(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    return actuals*np.log(predictions)+(np.ones(actuals.shape)-actuals)*np.log(np.ones(predictions.shape)-predictions)\n",
    "\n",
    "def cost_function(loss_function: Callable, predictions: np.ndarray, actuals: np.ndarray, weights: List[np.ndarray], decay_parameter: float) -> float:\n",
    "    avg_loss: float = np.mean(loss_function(predictions, actuals))\n",
    "    sum_layers = [np.power(layer_weight, 2).sum() for layer_weight in weights]\n",
    "    sum_all_weights = sum(sum_layers)\n",
    "    regularization: float = 0.5*decay_parameter*sum_all_weights\n",
    "    return avg_loss+regularization\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    weigths: np.ndarray\n",
    "    has_bias: bool\n",
    "\n",
    "    def __init__(self, num_nodes: int, num_nodes_n1: int, include_bias: bool = True, epsilon: float = 0.01) -> None:\n",
    "        self.weights = np.random.normal(loc=0, scale=np.power(epsilon,2), size=(num_nodes_n1, np.add(num_nodes, include_bias)))\n",
    "        self.has_bias = include_bias\n",
    "    \n",
    "    def print_weights(self) -> None:\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    layers: List[Layer]\n",
    "    weights: List[np.ndarray]\n",
    "\n",
    "    def __init__(self, num_nodes: List[int], include_biases: List[bool]) -> None:\n",
    "        # num_nodes is a list of number of nodes for all layers not counting the bias node\n",
    "        # TO-DO: code won't work if include_biases != [True, True, False], see prop_forward\n",
    "        assert (include_biases == [True, True, False]), 'error when initializing Network class: include_bias parameter not available'\n",
    "\n",
    "        self.layers = [Layer(num_nodes[i], num_nodes[i+1], include_biases[i]) for i in range(len(num_nodes)-1)]\n",
    "        self.layers.append(Layer(num_nodes[-1], 0, include_biases[-1]))\n",
    "        self.weights = self.get_weights(form='list')\n",
    "\n",
    "    def get_weights(self, form: str = 'vector') -> Union[List[np.ndarray], np.ndarray]:\n",
    "        assert (form in ['vector', 'list']), 'Error in get_weights function: form parameter ill-defined'\n",
    "\n",
    "        if form == 'vector':\n",
    "            # returns one np.ndarray with all weights of all layers\n",
    "            weigth_vector = []\n",
    "            for layer in self.layers:\n",
    "                weigth_vector.append(layer.weights)\n",
    "            return np.asarray(weigth_vector)\n",
    "        elif form == 'list':\n",
    "            # returns a list, where list[i] stores the weights between layer i-1 and layer i\n",
    "            list_of_weights: List[np.ndarray] = []\n",
    "            for layer in self.layers:\n",
    "                list_of_weights.append(layer.weights)\n",
    "            return list_of_weights\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        print('Printing weights of network:')\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f'Layer {index+1}')\n",
    "            layer.print_weights()\n",
    "\n",
    "    def prop_forward(self, features: np.ndarray) -> List[np.ndarray]:\n",
    "        # returns a list, where list[i] stores the activations for neurons in layer i+1\n",
    "        # the activation of a bias node (should the layer have one) is given by the first value in the array and is always =1\n",
    "        # TO-DO: right now it is hard coded that input layer & hidden layer have a bias node, but output layer has not\n",
    "        z_2 = np.matmul(self.weights[0], np.append(1, features))\n",
    "        a_2 = np.apply_along_axis(sigmoid, 0, z_2)\n",
    "        z_3 = np.matmul(self.weights[1], np.append(1, a_2))\n",
    "        a_3 = np.apply_along_axis(sigmoid, 0, z_3)\n",
    "        return [np.append(1, features), np.append(1, a_2), a_3]\n",
    "\n",
    "    def print_activations(self, features: np.ndarray) -> None:\n",
    "        print(f'Printing activations for input: {features}')\n",
    "        for index, array in enumerate(self.prop_forward(features)):\n",
    "            print(f'Layer {index+1}: {array}')\n",
    "\n",
    "    def calculate_z(self, num_layer: int, activations: List[np.ndarray]) -> float:\n",
    "        z = np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "        return np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "\n",
    "    def get_deltas(self, X: np.ndarray, y: np.ndarray, loss_function_derivative: Callable, activations: List[np.ndarray]=None) -> List[np.ndarray]:\n",
    "        # TO-DO: adapt code to accept different cost functions\n",
    "        # Right now: hard coded to use quadratic loss\n",
    "        if activations == None:\n",
    "            activations = self.prop_forward(X)\n",
    "        deltas = []\n",
    "        # Fehler: activations != z!!!!\n",
    "        # deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, activations[-1]))\n",
    "        deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z(3, activations)))\n",
    "        deltas.insert(0, deltas_output)\n",
    "        for i in range(len(activations)-2):\n",
    "            # delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, activations[-2]))\n",
    "            delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)][:,1:]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z((len(activations)-1-i), activations)))\n",
    "            # remove 'bias delta', as activation of bias cannot be changed\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "\n",
    "    def partial_derivatives(self, X: np.ndarray, y: np.ndarray, verbose: bool =False) -> List[np.ndarray]:\n",
    "        activations = self.prop_forward(X)\n",
    "        deltas = self.get_deltas(X, y, activations)\n",
    "        partial_derivatives = []\n",
    "        for index in range(len(deltas)):\n",
    "            if verbose:\n",
    "                # for testing/debugging purposes\n",
    "                print(f'Layer {index+1}: dimension deltas {index+2} {deltas[index].shape}, dimension activations {index+1} {activations[index].shape}')\n",
    "                print(f'activations: {activations[index]}')\n",
    "                print(f'deltas: {deltas[index]}')\n",
    "            partial = np.outer(deltas[index], np.transpose(activations[index]))\n",
    "            if verbose:\n",
    "                # deltas should be equal to partial derivatives of the bias node\n",
    "                print(f'Partial of bias: {partial[:,0]}')\n",
    "            partial_derivatives.append(partial)\n",
    "        return partial_derivatives\n",
    "\n",
    "    def update_weights(self, big_delta: List[np.ndarray], regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        for num_layer in range(len(self.weights)):\n",
    "            self.weights[num_layer] = self.weights[num_layer]-learning_rate*(big_delta[num_layer] + regularization_parameter*self.weights[num_layer])\n",
    "\n",
    "    def gradient_descent(self, X_train: np.ndarray, y_train: np.ndarray, regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        big_delta: List[np.ndarray] = [] \n",
    "        for index in range(len(self.weights)):\n",
    "            big_delta.append(np.zeros(self.weights[index].shape))\n",
    "        for num_instance in range(X_train.shape[0]):\n",
    "            partials = self.partial_derivatives(X[num_instance], y[num_instance])\n",
    "            for num_layer in range(len(self.layers)-1):\n",
    "                big_delta[num_layer] = big_delta[num_layer]+partials[num_layer]\n",
    "        for num_layer in range(len(self.layers)):\n",
    "            big_delta[num_layer] = (1/X_train.shape[0])*big_delta[num_layer]\n",
    "        self.update_weights(big_delta, regularization_parameter, learning_rate)\n",
    "\n",
    "    def train_network(self, n_iter: int, X_train: np.ndarray, y_train: np.ndarray, regul_param: float, learning_rate: float=0.01) -> None:\n",
    "        # TO-DO: print/plot loss after each iteration \n",
    "        \n",
    "        # calculate activation\n",
    "        # loss based on activation\n",
    "        costs = [[], []]\n",
    "        for iter in range(n_iter):\n",
    "            \n",
    "            temp_costs = []            \n",
    "            \n",
    "            for instance in X_train:\n",
    "                act_3 = self.prop_forward(instance)[2].reshape((8,1))\n",
    "                # print('shape act_3: ', act_3.shape, ' shape weights_3: ', weights_3.shape)\n",
    "                cos = cost_function(quadratic_loss, act_3, instance, self.weights, regul_param)\n",
    "                temp_costs.append(cos)\n",
    "                # print('predictions: ', act_3)\n",
    "            costs[0].append(iter)\n",
    "            costs[1].append(np.mean(temp_costs))\n",
    "            self.gradient_descent(X_train, y_train, regularization_parameter=regul_param, learning_rate=learning_rate)\n",
    "            \n",
    "        \n",
    "        plt.plot(costs[0], costs[1])\n",
    "        plt.show()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_network.print_weights()\n",
    "#test_network.print_activations(X[0])\n",
    "#print(test_network.get_deltas(X[0], X[0]))\n",
    "#print(test_network.partial_derivatives(X[0], X[0], verbose=False)[0])\n",
    "#test_network.gradient_descent(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network = Network([8,3,8], [True, True, False])\n",
    "test_network.train_network(2000, X, y, regul_param=0.001, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 0., 0., 0., 0., 0., 0., 0.]), array([1.        , 0.66887023, 0.66859227, 0.66890287]), array([0.13188531, 0.13188705, 0.1318868 , 0.13188534, 0.13188706,\n",
      "       0.1318873 , 0.13188711, 0.1318857 ])]\n"
     ]
    }
   ],
   "source": [
    "print(test_network.prop_forward(X[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06a35f6432bcea124c520d36814be75a6dd5ed4335e0c829924d510b7f0b7dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
