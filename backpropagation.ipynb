{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from typing import List, Union, Callable\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.identity(8)\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_network(input_size, hidden_size, output_size):\n",
    "    input = np.zeros((input_size, 2))\n",
    "    hidden = np.zeros((hidden_size, 2))\n",
    "    output = np.zeros((output_size, 2))\n",
    "    network = list((input, hidden, output))\n",
    "    return network\n",
    "network = init_network(8,3,8)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-4.38708623e-06, -4.16987629e-06,  2.47459412e-06, -6.07341737e-06]), array([-0.11750197,  0.11750583,  0.11749433,  0.11749259,  0.11749311,\n",
      "        0.11751673,  0.11749984,  0.11749342])]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def quadratic_loss(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    norms = np.apply_along_axis(np.linalg.norm, 0, predictions-actuals)\n",
    "    return 0.5*np.apply_along_axis(np.power, 0, norms, {'x2': 2})\n",
    "\n",
    "def cost_function(loss_function: Callable, predictions: np.ndarray, actuals: np.ndarray, weigths: np.ndarray, decay_parameter: float) -> float:\n",
    "    avg_loss: float = np.mean(loss_function(predictions, actuals))\n",
    "    regularization: float = 0.5*decay_parameter*sum(np.power(weights, 2))\n",
    "    return avg_loss+regularization\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    weigths: np.ndarray\n",
    "    has_bias: bool\n",
    "\n",
    "    def __init__(self, num_nodes: int, num_nodes_n1: int, include_bias: bool = True, epsilon: float = 0.01) -> None:\n",
    "        self.weights = np.random.normal(loc=0, scale=np.power(epsilon,2), size=(num_nodes_n1, np.add(num_nodes, include_bias)))\n",
    "        self.has_bias = include_bias\n",
    "    \n",
    "    def print_weights(self) -> None:\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    layers: List[Layer]\n",
    "\n",
    "    def __init__(self, num_nodes: List[int], include_biases: List[bool]) -> None:\n",
    "        # num_nodes is a list of number of nodes for all layers not counting the bias node\n",
    "        # TO-DO: code won't work if include_biases != [True, True, False], see prop_forward\n",
    "        assert (include_biases == [True, True, False]), 'error when initializing Network class: include_bias parameter not available'\n",
    "\n",
    "        self.layers = [Layer(num_nodes[i], num_nodes[i+1], include_biases[i]) for i in range(len(num_nodes)-1)]\n",
    "        self.layers.append(Layer(num_nodes[-1], 0, include_biases[-1]))\n",
    "\n",
    "    def get_weights(self, form: str = 'vector') -> Union[List[np.ndarray], np.ndarray]:\n",
    "        assert (form in ['vector', 'list']), 'Error in get_weights function: form parameter ill-defined'\n",
    "\n",
    "        if form == 'vector':\n",
    "            # returns one np.ndarray with all weights of all layers\n",
    "            weigth_vector = []\n",
    "            for layer in self.layers:\n",
    "                weigth_vector.append(layer.weights)\n",
    "            return np.asarray(weigth_vector)\n",
    "        elif form == 'list':\n",
    "            # returns a list, where list[i] stores the weights between layer i-1 and layer i\n",
    "            list_of_weights: List[np.ndarray] = []\n",
    "            for layer in self.layers:\n",
    "                list_of_weights.append(layer.weights)\n",
    "            return list_of_weights\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        print('Printing weights of network:')\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f'Layer {index+1}')\n",
    "            layer.print_weights()\n",
    "\n",
    "    def prop_forward(self, features: np.ndarray) -> List[np.ndarray]:\n",
    "        # returns a list, where list[i] stores the activations for neurons in layer i+1\n",
    "        # the activation of a bias node (should the layer have one) is given by the first value in the array and is always =1\n",
    "        # TO-DO: right now it is hard coded that input layer & hidden layer have a bias node, but output layer has not\n",
    "        z_2 = np.matmul(self.layers[0].weights, np.append(1, features))\n",
    "        a_2 = np.apply_along_axis(sigmoid, 0, z_2)\n",
    "        z_3 = np.matmul(self.layers[1].weights, np.append(1, a_2))\n",
    "        a_3 = np.apply_along_axis(sigmoid, 0, z_3)\n",
    "        return [np.append(1, features), np.append(1, a_2), a_3]\n",
    "\n",
    "    def print_activations(self, features: np.ndarray) -> None:\n",
    "        print(f'Printing activations for input: {features}')\n",
    "        for index, array in enumerate(self.prop_forward(features)):\n",
    "            print(f'Layer {index+1}: {array}')\n",
    "\n",
    "    def get_deltas(self, y: np.ndarray, activations: List[np.ndarray]=None) -> List[np.ndarray]:\n",
    "        # TO-DO: adapt code to accept different cost functions\n",
    "        # Right now: hard coded to use quadratic loss\n",
    "        if activations == None:\n",
    "            activations = self.prop_forward(y)\n",
    "        weights = self.get_weights(form='list')\n",
    "        deltas = []\n",
    "        deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, activations[-1]))\n",
    "        deltas.insert(0, deltas_output)\n",
    "        for i in range(len(activations)-2):\n",
    "            delta = np.multiply(np.matmul(np.transpose(weights[-(i+2)]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, activations[-2]))\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "\n",
    "    def partial_derivatives(self, y: np.ndarray, derivative: Callable) -> List[np.ndarray]:\n",
    "        activations = self.prop_forward(y)\n",
    "        deltas = self.get_deltas(y, derivative, activations)\n",
    "        partial_derivatives = []\n",
    "        for index in range(len(self.layers)):\n",
    "            next\n",
    "            # To-Do:\n",
    "        return\n",
    "\n",
    "    def prop_backward(self, learning_rate: float=0.01) -> None:\n",
    "        print('Performing backpropagation:')\n",
    "        print(f'Old weights: {self.weights}')\n",
    "        # TO-DO\n",
    "        print(f'New weights: {self.weights}')\n",
    "        return\n",
    "\n",
    "\n",
    "test_network = Network([8,3,8], [True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing weights of network:\n",
      "Layer 1\n",
      "[[-2.20416014e-05 -2.52834496e-05  1.49856583e-04 -1.15568236e-04\n",
      "  -4.54890157e-06  1.82793447e-04 -1.53982959e-04  8.41859305e-05\n",
      "   1.54359650e-05]\n",
      " [-3.47920980e-05  6.21081004e-05 -1.42811812e-05  4.37255526e-05\n",
      "  -1.19643270e-04  1.98613284e-05  1.33909314e-04 -4.57509756e-05\n",
      "   1.60639876e-04]\n",
      " [-2.26518058e-06  2.38637700e-05  1.31642177e-04 -1.18214784e-04\n",
      "  -1.34834938e-04  7.71115766e-05  2.24699369e-05  8.69052901e-05\n",
      "   7.02918260e-07]]\n",
      "Layer 2\n",
      "[[-1.76330414e-05  4.65598466e-06  5.69695344e-05 -2.97235847e-05]\n",
      " [ 1.29575556e-07 -2.43123818e-05  8.90422518e-05  8.91377601e-05]\n",
      " [ 8.00597873e-06 -1.69352609e-04 -6.95862093e-05 -6.91236761e-05]\n",
      " [-1.20472369e-04  9.54489746e-06 -4.41756895e-05 -8.39213582e-05]\n",
      " [-1.06473188e-04 -6.97281360e-05  1.07275408e-04 -1.63707986e-04]\n",
      " [ 2.34868690e-04 -1.73111288e-05  8.02324378e-05  4.45194563e-05]\n",
      " [-8.40050451e-05  7.73129844e-05  2.13757196e-05 -8.86488168e-06]\n",
      " [-1.39643333e-04  4.74859537e-05 -3.75938553e-05 -5.77441821e-05]]\n",
      "Layer 3\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_network.print_weights()\n",
    "#test_network.print_activations(X[0])\n",
    "#print(test_network.get_deltas(y=X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "def activation(inputs, weights, bias=True):\n",
    "    \n",
    "    if bias == True:\n",
    "        term = 1\n",
    "    else: \n",
    "        term = 0\n",
    "    for input, weight in zip(inputs, weights):\n",
    "        term += (input * weight)\n",
    "    return sigmoid(term)\n",
    "\n",
    "\n",
    "\n",
    "# activation function for top node hidden layer.\n",
    "print(activation([1,0,0,0,0,0,0,0], np.zeros((8,1)), False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, input):\n",
    "    for i in range(len(network)):\n",
    "        if i == 0:\n",
    "            network[i] = input\n",
    "        elif i == 1:\n",
    "            for node_i in range(len(network[i])):\n",
    "                network[i][node_i] = activation(network[i-1], np.zeros(network[i-1].shape), bias=False)\n",
    "        else: \n",
    "            for node_i in range(len(network[i])):\n",
    "                network[i][node_i] = activation(network[i-1], np.zeros(network[i-1].shape))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.62245933, 0.5       ])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid([1,0.5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8., 8.]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = np.ones((1,8))\n",
    "weights = np.ones((8,2))\n",
    "np.dot(activation, weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06a35f6432bcea124c520d36814be75a6dd5ed4335e0c829924d510b7f0b7dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
