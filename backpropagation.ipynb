{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from typing import List, Union, Callable\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.identity(8)\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z: float) -> float:\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def quadratic_loss(predictions: np.ndarray, actuals: np.ndarray) -> np.ndarray:\n",
    "    norms = np.apply_along_axis(np.linalg.norm, 0, predictions-actuals)\n",
    "    return 0.5*np.apply_along_axis(np.power, 0, norms, {'x2': 2})\n",
    "\n",
    "def cost_function(loss_function: Callable, predictions: np.ndarray, actuals: np.ndarray, weigths: np.ndarray, decay_parameter: float) -> float:\n",
    "    avg_loss: float = np.mean(loss_function(predictions, actuals))\n",
    "    regularization: float = 0.5*decay_parameter*sum(np.power(weights, 2))\n",
    "    return avg_loss+regularization\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    weigths: np.ndarray\n",
    "    has_bias: bool\n",
    "\n",
    "    def __init__(self, num_nodes: int, num_nodes_n1: int, include_bias: bool = True, epsilon: float = 0.01) -> None:\n",
    "        self.weights = np.random.normal(loc=0, scale=np.power(epsilon,2), size=(num_nodes_n1, np.add(num_nodes, include_bias)))\n",
    "        self.has_bias = include_bias\n",
    "    \n",
    "    def print_weights(self) -> None:\n",
    "        print(self.weights)\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    # declaration of instance variables\n",
    "    layers: List[Layer]\n",
    "    weights: List[np.ndarray]\n",
    "\n",
    "    def __init__(self, num_nodes: List[int], include_biases: List[bool]) -> None:\n",
    "        # num_nodes is a list of number of nodes for all layers not counting the bias node\n",
    "        # TO-DO: code won't work if include_biases != [True, True, False], see prop_forward\n",
    "        assert (include_biases == [True, True, False]), 'error when initializing Network class: include_bias parameter not available'\n",
    "\n",
    "        self.layers = [Layer(num_nodes[i], num_nodes[i+1], include_biases[i]) for i in range(len(num_nodes)-1)]\n",
    "        self.layers.append(Layer(num_nodes[-1], 0, include_biases[-1]))\n",
    "        self.weights = self.get_weights(form='list')\n",
    "\n",
    "    def get_weights(self, form: str = 'vector') -> Union[List[np.ndarray], np.ndarray]:\n",
    "        assert (form in ['vector', 'list']), 'Error in get_weights function: form parameter ill-defined'\n",
    "\n",
    "        if form == 'vector':\n",
    "            # returns one np.ndarray with all weights of all layers\n",
    "            weigth_vector = []\n",
    "            for layer in self.layers:\n",
    "                weigth_vector.append(layer.weights)\n",
    "            return np.asarray(weigth_vector)\n",
    "        elif form == 'list':\n",
    "            # returns a list, where list[i] stores the weights between layer i-1 and layer i\n",
    "            list_of_weights: List[np.ndarray] = []\n",
    "            for layer in self.layers:\n",
    "                list_of_weights.append(layer.weights)\n",
    "            return list_of_weights\n",
    "\n",
    "    def print_weights(self) -> None:\n",
    "        print('Printing weights of network:')\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f'Layer {index+1}')\n",
    "            layer.print_weights()\n",
    "\n",
    "    def prop_forward(self, features: np.ndarray) -> List[np.ndarray]:\n",
    "        # returns a list, where list[i] stores the activations for neurons in layer i+1\n",
    "        # the activation of a bias node (should the layer have one) is given by the first value in the array and is always =1\n",
    "        # TO-DO: right now it is hard coded that input layer & hidden layer have a bias node, but output layer has not\n",
    "        z_2 = np.matmul(self.weights[0], np.append(1, features))\n",
    "        a_2 = np.apply_along_axis(sigmoid, 0, z_2)\n",
    "        z_3 = np.matmul(self.weights[1], np.append(1, a_2))\n",
    "        a_3 = np.apply_along_axis(sigmoid, 0, z_3)\n",
    "        return [np.append(1, features), np.append(1, a_2), a_3]\n",
    "\n",
    "    def print_activations(self, features: np.ndarray) -> None:\n",
    "        print(f'Printing activations for input: {features}')\n",
    "        for index, array in enumerate(self.prop_forward(features)):\n",
    "            print(f'Layer {index+1}: {array}')\n",
    "\n",
    "    def calculate_z(self, num_layer: int, activations: List[np.ndarray]) -> float:\n",
    "        z = np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "        return np.dot(self.weights[num_layer-2], activations[num_layer-2])\n",
    "\n",
    "    def get_deltas(self, X: np.ndarray, y: np.ndarray, activations: List[np.ndarray]=None) -> List[np.ndarray]:\n",
    "        # TO-DO: adapt code to accept different cost functions\n",
    "        # Right now: hard coded to use quadratic loss\n",
    "        if activations == None:\n",
    "            activations = self.prop_forward(X)\n",
    "        deltas = []\n",
    "        # Fehler: activations != z!!!!\n",
    "        # deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, activations[-1]))\n",
    "        deltas_output = -np.multiply((y-activations[-1]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z(3, activations)))\n",
    "        deltas.insert(0, deltas_output)\n",
    "        for i in range(len(activations)-2):\n",
    "            # delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, activations[-2]))\n",
    "            delta = np.multiply(np.matmul(np.transpose(self.weights[-(i+2)][:,1:]), deltas[-(i+1)]), np.apply_along_axis(sigmoid_derivative, 0, self.calculate_z((len(activations)-1-i), activations)))\n",
    "            # remove 'bias delta', as activation of bias cannot be changed\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "\n",
    "    def partial_derivatives(self, X: np.ndarray, y: np.ndarray, verbose: bool =False) -> List[np.ndarray]:\n",
    "        activations = self.prop_forward(X)\n",
    "        deltas = self.get_deltas(X, y, activations)\n",
    "        partial_derivatives = []\n",
    "        for index in range(len(deltas)):\n",
    "            if verbose:\n",
    "                # for testing/debugging purposes\n",
    "                print(f'Layer {index+1}: dimension deltas {index+2} {deltas[index].shape}, dimension activations {index+1} {activations[index].shape}')\n",
    "                print(f'activations: {activations[index]}')\n",
    "                print(f'deltas: {deltas[index]}')\n",
    "            partial = np.outer(deltas[index], np.transpose(activations[index]))\n",
    "            if verbose:\n",
    "                # deltas should be equal to partial derivatives of the bias node\n",
    "                print(f'Partial of bias: {partial[:,0]}')\n",
    "            partial_derivatives.append(partial)\n",
    "        return partial_derivatives\n",
    "\n",
    "    def update_weights(self, big_delta: List[np.ndarray], regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        for num_layer in range(len(self.weights)):\n",
    "            self.weights[num_layer] = self.weights[num_layer]-learning_rate*(big_delta[num_layer] + regularization_parameter*self.weights[num_layer])\n",
    "\n",
    "    def gradient_descent(self, X_train: np.ndarray, y_train: np.ndarray, regularization_parameter: float, learning_rate: float=0.01) -> None:\n",
    "        big_delta: List[np.ndarray] = [] \n",
    "        for index in range(len(self.weights)):\n",
    "            big_delta.append(np.zeros(self.weights[index].shape))\n",
    "        for num_instance in range(X_train.shape[0]):\n",
    "            partials = self.partial_derivatives(X[num_instance], y[num_instance])\n",
    "            for num_layer in range(len(self.layers)-1):\n",
    "                big_delta[num_layer] = big_delta[num_layer]+partials[num_layer]\n",
    "        for num_layer in range(len(self.layers)):\n",
    "            big_delta[num_layer] = (1/X_train.shape[0])*big_delta[num_layer]\n",
    "        self.update_weights(big_delta, regularization_parameter, learning_rate)\n",
    "\n",
    "    def train_network(self, n_iter: int, X_train: np.ndarray, y_train: np.ndarray, regul_param: float, learning_rate: float=0.01) -> None:\n",
    "        # TO-DO: print/plot loss after each iteration \n",
    "        return\n",
    "\n",
    "\n",
    "test_network = Network([8,3,8], [True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = [-4.32393292e-05  5.51066172e-05 -2.07720375e-05  7.35849802e-05\n",
      "  2.22382290e-04  1.18510367e-04  1.25640236e-04 -7.44396280e-05]\n",
      "z = [ 8.66436715e-05  9.77925041e-05 -2.49304278e-04]\n",
      "z = [-4.32405188e-05  5.51084653e-05 -2.07713681e-05  7.35839223e-05\n",
      "  2.22383324e-04  1.18512077e-04  1.25641660e-04 -7.44397430e-05]\n",
      "z = [ 6.63491584e-05  1.34222694e-04 -2.27029525e-04]\n",
      "z = [-4.32412055e-05  5.51103728e-05 -2.07662659e-05  7.35814090e-05\n",
      "  2.22384757e-04  1.18512724e-04  1.25639149e-04 -7.44409101e-05]\n",
      "z = [-8.96626049e-06  1.11328720e-04 -2.15900462e-04]\n",
      "z = [-4.32426069e-05  5.51113252e-05 -2.07710808e-05  7.35824140e-05\n",
      "  2.22383924e-04  1.18514763e-04  1.25644235e-04 -7.44398398e-05]\n",
      "z = [ 3.48512641e-05  2.01404651e-04 -2.11544731e-04]\n",
      "z = [-4.32410336e-05  5.51088274e-05 -2.07752804e-05  7.35851729e-05\n",
      "  2.22384485e-04  1.18513638e-04  1.25646007e-04 -7.44386714e-05]\n",
      "z = [ 0.00012505  0.00019606 -0.00018287]\n",
      "z = [-4.32373877e-05  5.51038186e-05 -2.07721937e-05  7.35863311e-05\n",
      "  2.22381005e-04  1.18507595e-04  1.25637374e-04 -7.44396360e-05]\n",
      "z = [ 1.08897488e-04  3.07433730e-05 -2.80632799e-04]\n",
      "z = [-4.32467023e-05  5.51193821e-05 -2.07586773e-05  7.35746931e-05\n",
      "  2.22388780e-04  1.18519745e-04  1.25641742e-04 -7.44426404e-05]\n",
      "z = [-0.00017488  0.00023041 -0.00014974]\n",
      "z = [-4.32430034e-05  5.51109373e-05 -2.07763290e-05  7.35842676e-05\n",
      "  2.22383166e-04  1.18515678e-04  1.25648507e-04 -7.44386140e-05]\n",
      "z = [ 0.00010088  0.00026387 -0.00020514]\n"
     ]
    }
   ],
   "source": [
    "#test_network.print_weights()\n",
    "#test_network.print_activations(X[0])\n",
    "#print(test_network.get_deltas(X[0], X[0]))\n",
    "#print(test_network.partial_derivatives(X[0], X[0], verbose=False)[0])\n",
    "#test_network.gradient_descent(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(inputs, weights, bias=True):\n",
    "    \n",
    "    if bias == True:\n",
    "        term = 1\n",
    "    else: \n",
    "        term = 0\n",
    "    for input, weight in zip(inputs, weights):\n",
    "        term += (input * weight)\n",
    "    return sigmoid(term)\n",
    "\n",
    "\n",
    "\n",
    "# activation function for top node hidden layer.\n",
    "print(activation([1,0,0,0,0,0,0,0], np.zeros((8,1)), False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, input):\n",
    "    for i in range(len(network)):\n",
    "        if i == 0:\n",
    "            network[i] = input\n",
    "        elif i == 1:\n",
    "            for node_i in range(len(network[i])):\n",
    "                network[i][node_i] = activation(network[i-1], np.zeros(network[i-1].shape), bias=False)\n",
    "        else: \n",
    "            for node_i in range(len(network[i])):\n",
    "                network[i][node_i] = activation(network[i-1], np.zeros(network[i-1].shape))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid([1,0.5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = np.ones((1,8))\n",
    "weights = np.ones((8,2))\n",
    "np.dot(activation, weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d06a35f6432bcea124c520d36814be75a6dd5ed4335e0c829924d510b7f0b7dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
